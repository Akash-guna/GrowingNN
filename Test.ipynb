{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe613ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akash\\Anaconda3\\envs\\deit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import json\n",
    "\n",
    "from pathlib import Path\n",
    "import timm\n",
    "from timm.data import Mixup\n",
    "from timm.models import create_model\n",
    "from timm.loss import LabelSmoothingCrossEntropy, SoftTargetCrossEntropy\n",
    "from timm.scheduler import create_scheduler\n",
    "from timm.optim import create_optimizer\n",
    "from timm.utils import NativeScaler, get_state_dict, ModelEma\n",
    "\n",
    "from datasets import build_dataset\n",
    "from engine import train_one_epoch, evaluate\n",
    "from losses import DistillationLoss\n",
    "from samplers import RASampler\n",
    "from augment import new_data_aug_generator\n",
    "\n",
    "import models\n",
    "import models_v2\n",
    "from Growth import GrowthBlock\n",
    "from growth_utils import *\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "634af823",
   "metadata": {},
   "outputs": [],
   "source": [
    "from regression_utils_add import return_layers\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7551e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "path = os.path.dirname(timm.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff165b04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\akash\\\\FILES\\\\Research\\\\Growth\\\\Transformer\\\\deit-main\\\\timm'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e37ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(\n",
    "        \"deit_small_patch16_224\",\n",
    "        pretrained=False,\n",
    "        num_classes=1000,\n",
    "        drop_rate=0.0,\n",
    "        drop_path_rate=0.1,\n",
    "        drop_block_rate=None,\n",
    "        img_size=224\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26550b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11417704"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef994b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75c9689f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = torch.rand(1,3,224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29661540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.9423e-01,  1.0607e-01, -4.2674e-01,  3.5378e-01, -3.0509e-01,\n",
       "         -3.5112e-01, -1.0366e-01, -2.7353e-01, -3.9596e-01, -1.7731e-01,\n",
       "         -2.3314e-01,  1.7888e-01,  1.6268e-01,  3.8845e-01, -4.7584e-01,\n",
       "         -5.0599e-01, -4.8835e-02, -7.9453e-02,  3.0131e-01, -5.0275e-01,\n",
       "          1.1758e-01, -1.4041e+00,  9.8829e-02,  4.0085e-01, -3.7122e-02,\n",
       "         -5.9900e-02, -3.2752e-01,  3.1380e-01,  1.9434e-01,  3.9211e-01,\n",
       "          4.6031e-01, -3.5910e-01, -8.9669e-01, -5.2335e-01, -3.5155e-01,\n",
       "         -3.3317e-01,  1.2243e-01, -7.1191e-02, -2.5903e-01, -1.9185e-01,\n",
       "         -1.4236e-01, -2.8565e-01, -6.0403e-02, -2.9026e-01, -2.5405e-03,\n",
       "         -2.8210e-01, -6.2894e-02,  5.1871e-01,  4.0110e-01,  3.1361e-01,\n",
       "         -3.0044e-01,  3.9687e-01, -1.4883e-01,  1.3035e-01,  7.2563e-02,\n",
       "         -2.3867e-01,  1.2066e-01, -1.2972e-02,  1.0374e+00, -1.0325e-01,\n",
       "         -7.5619e-02,  1.6402e-01,  3.0383e-04,  3.0912e-02,  1.5658e-01,\n",
       "          1.5838e-01, -4.0331e-01, -2.5171e-01,  1.9515e-01,  4.4028e-01,\n",
       "          5.8812e-01,  4.7595e-01, -5.0841e-01, -5.3822e-01,  8.5109e-01,\n",
       "          3.3784e-01,  3.8094e-01, -6.6043e-01,  1.5839e-01, -4.2189e-01,\n",
       "         -1.2703e-01, -2.9863e-01,  2.4117e-03,  3.0571e-01, -8.8593e-01,\n",
       "         -3.4896e-01, -3.7147e-01,  5.0969e-02,  5.3626e-01,  4.7592e-01,\n",
       "         -8.1993e-02,  2.0988e-01,  3.5309e-01,  2.7617e-01, -6.4579e-01,\n",
       "         -5.6312e-01, -2.9750e-01, -2.1313e-01, -9.7553e-02,  1.2001e-01,\n",
       "          1.2069e-01,  1.1956e-01,  3.8221e-01,  6.0541e-01, -6.2666e-02,\n",
       "          3.2596e-01, -5.6360e-02, -2.6810e-01, -2.2478e-01,  2.4644e-01,\n",
       "          1.4412e-02, -9.8323e-02, -3.3817e-01,  2.3591e-01,  5.2416e-01,\n",
       "          3.9852e-01, -8.0310e-01, -1.0288e-01,  7.6081e-02,  5.0446e-01,\n",
       "          8.0343e-02,  1.1540e-01, -6.2936e-03,  2.9870e-01, -4.2480e-02,\n",
       "          2.3506e-01,  2.3077e-01,  5.3439e-01,  1.0238e-03,  5.6133e-01,\n",
       "          2.7549e-01,  2.0960e-01,  4.6496e-01,  4.6993e-01,  4.9772e-02,\n",
       "          1.9097e-01,  6.4918e-01,  4.5815e-02, -1.0207e+00, -5.1421e-01,\n",
       "          6.9676e-03,  1.8496e-01,  7.3296e-01, -4.8161e-01, -1.8477e-01,\n",
       "          5.7145e-02, -3.4995e-01, -3.1397e-01,  2.0069e-01, -9.2502e-02,\n",
       "         -1.6744e-01, -2.4951e-01,  1.7054e-01, -3.7053e-02,  5.8900e-02,\n",
       "         -1.3836e-01, -3.8520e-01, -1.3047e-01, -3.9509e-01,  7.7125e-01,\n",
       "         -6.5651e-03,  7.2511e-02,  2.9068e-01,  3.0665e-01,  1.6760e-01,\n",
       "         -3.6861e-01,  2.6613e-01,  3.4670e-01, -2.9145e-03, -5.9683e-01,\n",
       "         -5.7797e-01,  5.7965e-01, -3.4489e-02, -3.9169e-01,  3.4172e-01,\n",
       "          2.3944e-01, -5.3447e-01, -2.3997e-01, -6.9354e-01, -1.3803e-01,\n",
       "         -3.0367e-01, -6.3374e-02, -5.2951e-01,  1.3061e-01,  6.7415e-01,\n",
       "         -2.3030e-02, -3.1690e-01, -3.5683e-02, -1.5236e-01,  4.4755e-01,\n",
       "         -2.8667e-01,  4.4162e-01,  1.1720e-01, -7.2882e-01,  4.2993e-01,\n",
       "          3.0268e-01, -4.1816e-01, -5.1015e-01,  1.7903e-01,  2.5509e-01,\n",
       "         -5.4630e-01, -1.4557e-03, -4.3295e-01, -2.9973e-01, -1.5102e-01,\n",
       "          3.2149e-03, -3.9902e-01, -5.4875e-01, -8.7733e-02,  6.1660e-02,\n",
       "         -1.2261e-01, -7.4941e-01, -1.2656e-01,  9.6512e-01, -5.1712e-01,\n",
       "          2.6301e-01,  1.6729e-01,  1.0386e+00, -5.7662e-01,  9.4224e-01,\n",
       "         -2.7415e-01,  4.2615e-01, -2.0771e-01,  5.6833e-01,  2.1210e-01,\n",
       "         -3.8239e-01, -4.0743e-02,  9.8988e-01,  3.9128e-02,  1.1663e-01,\n",
       "          4.9887e-01, -3.3657e-01,  5.0440e-01, -1.7755e-02,  3.7510e-01,\n",
       "         -3.6575e-01,  3.4820e-01, -2.7438e-01,  2.8160e-01,  1.7129e-01,\n",
       "          1.5935e-01, -1.7302e-01, -6.7105e-01,  6.3609e-01,  1.3354e-01,\n",
       "         -8.1037e-01, -1.8640e-01,  3.0047e-01, -2.8100e-01,  2.8859e-01,\n",
       "          3.6022e-01, -1.9489e-01,  2.5874e-01,  2.1898e-01,  2.5085e-01,\n",
       "         -2.7723e-01,  3.9168e-01, -8.2224e-01,  2.2072e-01, -5.4874e-01,\n",
       "         -3.6897e-01,  1.4894e-01,  4.6911e-02, -4.7195e-01, -1.7432e-02,\n",
       "         -4.2914e-01,  1.6021e-01,  6.2166e-02, -2.3771e-01,  3.7009e-01,\n",
       "          2.0867e-01,  8.3181e-01,  6.1641e-01,  9.3530e-02, -1.5114e-01,\n",
       "         -1.1847e-01, -2.8793e-01,  2.5751e-01,  1.4569e-01,  2.5061e-01,\n",
       "          1.8692e-02,  7.2731e-01,  7.9251e-02,  1.5629e-01, -2.8904e-01,\n",
       "          5.7989e-01,  1.5833e-02, -1.8747e-01,  4.2112e-01,  2.7125e-02,\n",
       "         -5.9235e-01,  1.5404e-01, -1.1614e-01,  1.4932e-01,  2.9214e-01,\n",
       "          4.2559e-01,  5.3053e-01, -3.3970e-01, -3.2003e-01,  6.7626e-01,\n",
       "         -5.6237e-01,  5.0459e-01, -2.2357e-01,  9.5373e-02, -3.3428e-01,\n",
       "         -8.9994e-01, -6.3384e-02, -3.1082e-01, -3.0707e-01, -2.5226e-01,\n",
       "         -1.9437e-01, -2.3232e-01,  1.7498e-02, -6.3145e-01, -3.1133e-01,\n",
       "         -3.2172e-01,  3.1467e-01, -1.2128e-01,  2.8867e-01,  1.1791e-01,\n",
       "         -2.2522e-01,  4.5711e-01, -5.7460e-02,  1.0005e-01,  3.4916e-01,\n",
       "          3.1603e-01,  2.8689e-01,  6.1597e-01,  3.7682e-01,  4.2000e-01,\n",
       "         -1.8947e-01, -1.5927e-01, -5.1063e-01, -3.3605e-01, -7.1419e-01,\n",
       "         -4.7537e-01,  6.1268e-01, -7.2391e-02, -2.0099e-01, -1.6932e-01,\n",
       "         -1.6288e-01, -9.2742e-02, -1.5639e-03,  2.1261e-01, -5.3718e-01,\n",
       "          2.2915e-01,  6.0139e-01,  5.1986e-01,  3.1800e-01, -2.3756e-02,\n",
       "         -9.9843e-02, -1.1231e+00, -2.5317e-02, -2.3700e-01, -5.9045e-01,\n",
       "          4.7686e-01,  1.5965e-01, -1.0891e+00,  2.6952e-02, -7.7713e-01,\n",
       "          7.0249e-02,  2.2142e-01, -3.0087e-01,  2.0749e-01, -2.8039e-01,\n",
       "          1.9729e-01, -2.6465e-01, -6.6516e-01,  9.5412e-02,  6.1325e-01,\n",
       "         -1.7581e-01, -6.8780e-01,  5.7271e-01, -5.2080e-01, -8.1103e-01,\n",
       "          2.4803e-02,  5.3703e-01,  6.3982e-01, -2.9795e-01,  1.2722e-01,\n",
       "         -2.1758e-01, -3.6590e-02, -6.5632e-01,  8.4072e-01,  5.0851e-02,\n",
       "         -1.8527e-01,  3.0427e-01,  1.9091e-01, -3.3467e-01, -8.1270e-01,\n",
       "          4.0264e-01, -5.9010e-01, -6.1904e-01, -7.8538e-02,  5.3421e-01,\n",
       "         -2.7733e-01,  3.6381e-02, -4.9761e-01,  5.4132e-01, -4.1094e-01,\n",
       "         -1.3670e-01,  5.2267e-01,  2.3831e-01, -2.7410e-01,  4.7773e-02,\n",
       "         -3.8624e-03,  2.3614e-02,  4.6212e-01,  2.1103e-01, -2.5949e-01,\n",
       "          5.9746e-03, -2.2273e-01,  7.1796e-02, -3.7183e-01,  2.6002e-01,\n",
       "          3.3538e-02,  1.5152e-01,  8.5116e-02, -5.9292e-01, -3.1753e-01,\n",
       "         -1.9721e-01,  1.1994e+00,  3.4341e-01, -1.1481e-01,  1.1638e-01,\n",
       "         -3.7338e-01, -1.0491e-02, -4.5186e-01,  3.7641e-01, -1.1860e-01,\n",
       "          5.2019e-01, -2.5783e-01,  1.0500e+00, -2.5728e-01, -4.6035e-01,\n",
       "         -1.6092e-01, -4.3474e-02,  2.4092e-02,  1.2320e-01,  1.0294e+00,\n",
       "         -4.7389e-01,  3.6258e-01,  4.1627e-02,  2.3643e-01, -3.2780e-01,\n",
       "          1.4626e-01, -3.0176e-01,  3.2825e-01,  8.6012e-01, -5.7446e-01,\n",
       "          3.0009e-01, -4.8766e-01,  5.9709e-02,  1.0437e-01,  4.1190e-01,\n",
       "          2.6156e-01, -3.5387e-01, -5.9111e-02, -1.6838e-01, -7.2514e-01,\n",
       "         -6.4812e-01, -3.7985e-01,  4.7380e-01,  2.6700e-01,  2.8726e-02,\n",
       "         -3.6021e-01,  5.1830e-01, -4.9725e-01, -3.4325e-01,  1.8142e-01,\n",
       "         -5.4772e-01,  1.9504e-01,  2.3621e-01, -2.7964e-01,  2.7726e-01,\n",
       "         -1.1531e-01, -3.8427e-01, -4.4701e-01,  5.5142e-01,  7.4327e-02,\n",
       "         -9.7069e-02,  3.6689e-01,  4.2717e-03,  4.6788e-01,  9.9449e-02,\n",
       "          2.6272e-01,  5.5325e-01, -4.1488e-02, -8.6917e-01,  4.5848e-01,\n",
       "         -1.3072e-01,  1.0163e-01, -3.6074e-01,  1.0241e+00,  8.8236e-02,\n",
       "         -4.9229e-01,  4.8298e-01,  2.8233e-01,  7.1440e-01, -4.0091e-01,\n",
       "          1.0379e-01, -4.6886e-01, -1.2118e-01, -3.0977e-01,  3.3315e-01,\n",
       "         -1.2312e-01, -1.6607e-01, -5.1984e-01,  4.7537e-01, -4.2196e-02,\n",
       "         -5.5526e-02,  1.9728e-01,  3.6007e-01,  1.3739e-02, -2.8110e-01,\n",
       "          2.2386e-01,  4.1941e-02, -2.2497e-01,  7.2717e-01, -1.7681e-01,\n",
       "          3.8285e-01, -1.6123e-01,  3.7567e-01,  6.0689e-01,  3.6087e-01,\n",
       "          1.2589e-01,  3.0716e-01, -1.5998e-01,  1.0563e+00,  3.1521e-01,\n",
       "          3.7048e-01,  6.4797e-01,  6.4507e-01, -5.9648e-01,  4.4804e-02,\n",
       "         -2.0730e-01, -7.0865e-01, -5.7836e-01, -8.4614e-02, -8.9871e-02,\n",
       "          8.5240e-01,  3.9948e-01,  2.2374e-01, -5.1113e-02,  2.1549e-01,\n",
       "         -2.9739e-01, -3.1093e-01, -7.8615e-02, -2.9621e-01, -9.6130e-02,\n",
       "          4.0818e-01,  1.8784e-01,  4.2274e-01, -1.8152e-01,  3.8486e-01,\n",
       "         -4.9298e-01, -2.1701e-01,  3.4519e-01,  6.6468e-02, -7.8324e-01,\n",
       "          3.1684e-01,  2.6457e-02, -2.2969e-01,  9.2792e-01, -7.2596e-01,\n",
       "         -5.0742e-01, -3.5077e-01, -1.2276e-01,  3.0828e-01, -1.0625e-01,\n",
       "          8.8961e-02,  1.5104e-01,  5.9376e-01,  4.4825e-01, -2.5411e-01,\n",
       "         -9.8014e-01, -1.0806e-01,  1.1765e-01, -3.3293e-01,  7.1700e-01,\n",
       "         -4.5521e-01, -2.9742e-01, -4.7649e-01, -2.0452e-01,  3.0048e-01,\n",
       "          1.7986e-01, -1.7218e-01, -3.4723e-02, -1.0680e-01,  3.5735e-01,\n",
       "         -4.7074e-01, -1.0824e-01,  1.2170e-01, -3.6947e-01,  2.0419e-02,\n",
       "          1.9675e-01,  4.6911e-02, -5.9137e-01,  8.6472e-04,  1.3836e-01,\n",
       "         -6.6470e-02,  3.3614e-01, -1.5760e-01, -2.1135e-01,  3.2607e-01,\n",
       "         -3.0866e-01,  4.9585e-01, -8.3931e-03,  7.5928e-01, -9.0657e-01,\n",
       "          3.2802e-01,  1.2319e-01,  3.5471e-01, -1.1644e-01, -3.3442e-01,\n",
       "         -1.2202e-01,  9.1546e-02,  3.0581e-02, -2.9434e-01, -4.2085e-01,\n",
       "         -2.3235e-01, -7.7561e-02,  3.5595e-01, -3.0997e-01, -3.7399e-01,\n",
       "          4.5128e-01, -1.5101e-01,  5.7311e-01, -2.3932e-01, -3.4192e-01,\n",
       "          1.5522e-01, -4.5199e-02,  3.2117e-02, -3.3419e-01, -7.6955e-02,\n",
       "         -2.3496e-02, -3.3682e-01, -1.4806e-01,  2.9701e-01, -1.8688e-01,\n",
       "         -2.2779e-01,  2.3099e-01, -4.7943e-01,  6.3156e-01, -2.8258e-01,\n",
       "          6.1933e-01, -1.7351e-01,  3.7142e-02,  1.9179e-01,  1.5678e-01,\n",
       "         -4.0011e-02,  2.5667e-01, -6.0213e-01, -3.8157e-01,  3.1612e-01,\n",
       "          4.4095e-01,  6.8593e-02, -1.8706e-01, -2.5301e-01,  5.7192e-03,\n",
       "          8.4677e-01, -7.2881e-01,  1.6414e-01, -6.8044e-01, -2.8283e-01,\n",
       "         -4.1107e-01,  4.4461e-01, -1.2845e-01,  3.6739e-01, -8.0161e-01,\n",
       "         -2.5419e-01, -1.8163e-01,  1.7903e-01,  5.5733e-01, -5.4789e-01,\n",
       "          2.9949e-01, -1.8972e-01, -9.8240e-02, -4.3619e-01,  1.1050e+00,\n",
       "         -1.1908e-01,  3.6081e-01, -2.8402e-01, -6.9238e-03,  4.6517e-01,\n",
       "         -3.2866e-01,  3.6258e-01,  4.0516e-01,  8.1020e-01, -6.0553e-01,\n",
       "         -6.3009e-01,  1.8332e-01, -4.6145e-01, -2.2202e-02,  5.3562e-02,\n",
       "         -3.1997e-01, -5.3628e-01, -3.8537e-01,  1.9793e-01, -3.3906e-01,\n",
       "          6.4553e-01, -1.0598e+00,  1.7706e-01, -3.6600e-01, -2.8469e-01,\n",
       "          6.2323e-01, -2.3943e-01, -1.9929e-01,  2.0563e-01, -5.2679e-01,\n",
       "         -3.6051e-01, -5.2591e-01,  1.0049e+00, -3.3488e-01, -2.0828e-01,\n",
       "          6.3491e-01,  3.2904e-01, -7.4265e-03, -5.4782e-01,  2.6966e-01,\n",
       "         -3.4488e-02,  6.4823e-01, -1.1896e-01, -4.0060e-01,  2.7743e-01,\n",
       "          8.9969e-02,  1.3125e-01,  4.4009e-01,  2.0638e-01, -1.2418e-01,\n",
       "          4.3465e-01, -9.5720e-02, -3.9404e-01,  2.3716e-01, -4.3032e-01,\n",
       "         -2.3753e-01,  4.9114e-01,  8.2844e-01, -9.9125e-01,  6.8119e-02,\n",
       "          3.8388e-01,  4.3893e-01,  5.7117e-01,  2.9941e-01, -1.4096e-01,\n",
       "          6.4792e-03, -5.1944e-01, -2.7381e-01, -3.1472e-02, -3.9073e-01,\n",
       "          4.4692e-01,  5.6840e-01,  2.0319e-02, -3.3653e-01, -8.3529e-02,\n",
       "         -4.5024e-02,  8.0499e-01,  5.4046e-01, -6.0812e-02,  5.7538e-03,\n",
       "         -6.6290e-01,  7.8487e-01,  3.4956e-01, -2.1328e-01, -1.1395e-01,\n",
       "         -7.3210e-01,  2.8448e-01, -1.3353e-01,  8.8782e-02, -3.7463e-01,\n",
       "         -3.0150e-01, -6.1221e-01,  4.8624e-01, -1.7733e-01,  2.1324e-01,\n",
       "          3.1172e-01, -2.2851e-02,  1.8743e-01, -5.3553e-01, -6.5727e-01,\n",
       "         -1.6116e-01, -3.4610e-01,  3.8137e-01, -3.4533e-01,  4.5687e-01,\n",
       "          4.3000e-01, -8.0313e-02,  3.2872e-01, -9.6398e-02, -1.4175e-02,\n",
       "          4.2767e-01, -4.6748e-01, -4.0699e-03,  2.9517e-01, -9.9358e-01,\n",
       "          4.4391e-01, -2.0527e-01,  1.0067e-01,  4.5499e-02,  1.8902e-02,\n",
       "          7.5286e-02,  9.6411e-03,  8.2166e-01,  2.0055e-01,  5.2299e-01,\n",
       "         -3.3643e-01, -3.9812e-01, -7.4820e-02, -9.4425e-03,  2.3647e-01,\n",
       "          1.0452e+00,  2.7439e-01, -8.5219e-02,  1.3351e-01,  6.9250e-01,\n",
       "          1.9076e-01,  9.5222e-02,  2.6213e-01, -6.4373e-01, -3.1442e-01,\n",
       "         -3.8236e-01, -1.2755e-01,  1.8233e-01, -3.2724e-01, -1.8442e-01,\n",
       "         -9.3438e-02,  9.6466e-02, -3.4805e-01, -1.0076e-01,  3.1067e-01,\n",
       "         -2.3130e-01,  3.5599e-02, -1.2547e-01, -3.5499e-01, -1.6233e-01,\n",
       "         -7.3587e-01, -7.9885e-01, -4.0189e-01, -1.0810e-01, -2.4895e-01,\n",
       "          8.7617e-02, -3.5644e-02,  3.1946e-02,  1.0111e-01, -3.1495e-01,\n",
       "         -5.9527e-02,  4.4254e-01, -1.1802e-01,  3.7606e-01, -1.7755e-01,\n",
       "         -3.5731e-02,  9.1961e-02,  2.3565e-01,  9.6888e-02, -5.1189e-01,\n",
       "          4.0988e-01, -1.5076e-01,  3.1437e-01,  1.1783e+00, -3.6038e-01,\n",
       "          2.4004e-01,  8.8049e-02, -1.8892e-01, -3.8905e-01,  5.0899e-01,\n",
       "         -5.9627e-02,  3.0921e-01, -1.9385e-01, -6.9102e-01, -1.2223e-01,\n",
       "          1.0624e-01, -4.6002e-02,  3.9445e-02, -2.3366e-01,  5.1973e-01,\n",
       "         -3.0642e-01,  3.2967e-01,  3.0088e-01, -3.0792e-01, -3.4541e-01,\n",
       "          7.9922e-01, -2.7409e-01, -1.2538e-02,  2.9485e-01, -3.3087e-01,\n",
       "         -4.8166e-01,  1.1309e-01,  2.2916e-01,  8.4223e-02, -2.4739e-01,\n",
       "          1.6087e-01, -8.5788e-02,  5.9561e-02, -1.1331e-01,  4.6821e-02,\n",
       "         -5.6741e-01, -2.9925e-01,  3.7310e-01, -5.3231e-01, -3.2874e-01,\n",
       "         -7.3332e-01,  1.3461e-02, -1.7690e-01,  1.2036e-01,  2.0762e-02,\n",
       "          2.5993e-01,  5.2000e-01, -7.7758e-02, -1.6591e-01,  4.7608e-01,\n",
       "         -7.0172e-01, -3.7270e-01,  2.3978e-01, -3.6059e-01,  2.3069e-01,\n",
       "         -2.4179e-01,  8.0852e-01, -6.9553e-02, -5.0978e-01, -1.7262e-01,\n",
       "         -1.0808e-01,  5.4416e-01,  1.7989e-01, -3.9789e-01, -3.9405e-01,\n",
       "         -1.9336e-01, -4.3745e-01,  2.8931e-02, -3.9731e-01,  1.2130e+00,\n",
       "          4.4516e-01,  1.6627e-01,  2.1836e-01,  9.1038e-02, -2.3567e-01,\n",
       "         -3.1957e-01, -1.0009e-01,  2.3999e-01,  5.9663e-01,  3.3002e-01,\n",
       "          2.5029e-01, -2.4329e-01, -8.9268e-02, -2.4084e-01,  1.1493e-01,\n",
       "         -2.4227e-01, -7.5953e-03,  2.7313e-01,  1.8956e-01,  4.2347e-02,\n",
       "          1.8718e-01,  3.2599e-01, -3.0757e-01,  1.3157e-01, -1.8124e-02,\n",
       "          5.1744e-01, -3.6659e-01, -2.2768e-01,  5.2485e-01,  4.1185e-01,\n",
       "         -4.8863e-01,  5.9165e-01, -4.6769e-01,  1.9461e-01, -3.8530e-01,\n",
       "          9.9730e-02,  1.8800e-02, -3.8656e-01,  3.5116e-01, -8.5982e-01,\n",
       "         -4.0887e-01, -2.0299e-02,  1.7638e-01, -1.2853e-02, -4.7474e-01,\n",
       "         -1.7862e-01, -2.3545e-01,  6.2012e-01, -7.1007e-01,  1.3584e-01,\n",
       "         -1.6125e-01,  5.0515e-01, -2.6808e-01, -2.1768e-01,  2.3730e-01,\n",
       "          4.2603e-01,  3.2523e-01, -6.8576e-01, -8.6530e-02, -3.1796e-02,\n",
       "          2.3735e-01, -2.9612e-01, -5.7259e-01,  6.0158e-01,  5.8090e-02,\n",
       "          4.4437e-01, -2.7667e-01, -1.2273e-01, -1.5860e-01, -4.0113e-01]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(rand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3becd197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9da11ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=torch.load(\"splitted model3.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c63eb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image =  cv2.imread(r\"C:\\Users\\akash\\FILES\\Research\\intrinsic\\Datasets\\tiny-imagenet-200\\train\\n01629819\\images\\n01629819_0.JPEG\")\n",
    "image = cv2.resize(image,(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db6bfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=torch.Tensor(np.expand_dims(image,axis=0)).permute([0,3,1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "758dcc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bf3f923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(20289604.)\n"
     ]
    }
   ],
   "source": [
    "o1=model(img)\n",
    "print(torch.sum(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0263ab21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def growth_wrapper(model):\n",
    "    for i in range(len(model.blocks)):\n",
    "        #print(f\"Block {i} QKV {model.blocks[i].attn.qkv}\")\n",
    "        model.blocks[i].attn.qkv = GrowthBlock(model.blocks[i].attn.qkv,act_on=False)\n",
    "        #print(f\"Block {i} proj {model.blocks[i].attn.proj}\")\n",
    "        model.blocks[i].attn.proj = GrowthBlock(model.blocks[i].attn.proj,act_on=False)\n",
    "        #print(f\"Block {i} MLP fc1 {model.blocks[i].mlp.fc1}\")\n",
    "        model.blocks[i].mlp.fc1 = GrowthBlock(model.blocks[i].mlp.fc1)\n",
    "        #print(f\"Block {i} MLP fc2 {model.blocks[i].mlp.fc2}\")\n",
    "        model.blocks[i].mlp.fc2 = GrowthBlock(model.blocks[i].mlp.fc2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4164d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=growth_wrapper(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b868feec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_linear_layers_transformer(model):\n",
    "    l=[]\n",
    "    l_att =[]\n",
    "    for i in range(len(model.blocks)):\n",
    "        #print(f\"Block {i} QKV {model.blocks[i].attn.qkv}\")\n",
    "        a=get_all_linear_layers(model.blocks[i].attn.qkv,typ='list')\n",
    "        l+=a\n",
    "        a_att = [[i,0,j] for j in range(len(a))]\n",
    "        l_att+=a_att\n",
    "        b=get_all_linear_layers(model.blocks[i].attn.proj,typ='list')\n",
    "        l+=b\n",
    "        b_att = [[i,1,j] for j in range(len(b))]\n",
    "        l_att+=b_att\n",
    "        c=get_all_linear_layers(model.blocks[i].mlp.fc1,typ='list')\n",
    "        l+=c\n",
    "        c_att = [[i,2,j] for j in range(len(c))]\n",
    "        l_att+=c_att\n",
    "        d=get_all_linear_layers(model.blocks[i].mlp.fc2,typ='list')\n",
    "        l+=d\n",
    "        d_att = [[i,3,j] for j in range(len(d))]\n",
    "        l_att+=d_att\n",
    "        #model.blocks[i].attn.qkv = GrowthBlock(model.blocks[i].attn.qkv,act_on=False)\n",
    "        #print(f\"Block {i} proj {model.blocks[i].attn.proj}\")\n",
    "        #model.blocks[i].attn.proj = GrowthBlock(model.blocks[i].attn.proj,act_on=False)\n",
    "        #print(f\"Block {i} MLP fc1 {model.blocks[i].mlp.fc1}\")\n",
    "        #model.blocks[i].mlp.fc1 = GrowthBlock(model.blocks[i].mlp.fc1)\n",
    "        #print(f\"Block {i} MLP fc2 {model.blocks[i].mlp.fc2}\")\n",
    "        #model.blocks[i].mlp.fc2 = GrowthBlock(model.blocks[i].mlp.fc2)\n",
    "    return l,l_att"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3510df69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_all_eigs(layers):\n",
    "    eigs=[]\n",
    "    for i,layer in enumerate(layers):\n",
    "        gradient = layer.weight\n",
    "        splitting = split_matrix(gradient)\n",
    "        #print(splitting.shape)\n",
    "        eig = splitting.eig()\n",
    "        eigs.append(torch.min(eig,axis=0).values)\n",
    "    return torch.Tensor(eigs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cd5fac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c24d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "l,la=get_all_linear_layers_transformer(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ac99987",
   "metadata": {},
   "outputs": [],
   "source": [
    "l,la=get_all_linear_layers_transformer(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b3ecf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_split_layers(model,percent=20,layer_percent=20):\n",
    "    l,la=get_all_linear_layers_transformer(model)\n",
    "    eigs = calc_all_eigs(l)\n",
    "    r,c = eigs.shape\n",
    "    rank = eigs.flatten().argsort()\n",
    "    limit = (rank.shape(0)/100)*percent\n",
    "    d={}\n",
    "    for i in range(limit):\n",
    "        pos = rank[i]\n",
    "        layer_num = pos//len(l)\n",
    "        try:\n",
    "            d[str(layer_num)]+=1\n",
    "        except:         \n",
    "            d[str(layer_num)]=1\n",
    "    sel_layer_data=[]                                                                                                                                                                                                                                                                                                                                                                                                           \n",
    "    for k,v in d.items():\n",
    "        layer = l[int(k)]\n",
    "        if v >= layer.out_nodes * (layer_percent/100):\n",
    "            sel_layer_data.append([l[int(k)]]+la[int(k)]+[v])\n",
    "    return sel_layer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_growth_model(block, num):\n",
    "    if num==0:\n",
    "        return block.attn.qkv\n",
    "    elif num==1:\n",
    "        return block.attn.proj\n",
    "    elif num==2:\n",
    "        return block.mlp.fc1\n",
    "    else:\n",
    "        return block.mlp.fc2\n",
    "\n",
    "def assign_model(model,block,num,gb):\n",
    "    if num==0:\n",
    "        model.blocks[block].attn.qkv =gb\n",
    "    elif num==1:\n",
    "        model.blocks[block].attn.proj = gb\n",
    "    elif num==2:\n",
    "        model.blocks[block].mlp.fc1 = gb\n",
    "    else:\n",
    "        model.blocks[block].mlp.fc2 = gb\n",
    "    return model\n",
    "\n",
    "def split_nodewise(model,percent=20,layer_percent=20,act_on=True):\n",
    "    sel_layers_data=find_split_layers(model)\n",
    "    for layer_data in sel_layers_data:\n",
    "        _,_,neg_index=calculate_max_layer([layer_data[0]])\n",
    "        neg_index = neg_index[:layer_data[-1]]\n",
    "        perm = permutation(choices,layer.out_features)\n",
    "        block = layer_data[1]\n",
    "        growth_block =layer_data[2]\n",
    "        layers = get_all_linear_layers(ret_growth_model(model.blocks[block],growth_block))\n",
    "        layer=layer_data[0]\n",
    "        s_layer=None\n",
    "        for i,l in enumerate(layers):\n",
    "            if l==layer_data[0]:\n",
    "                layers=layers.pop(i)\n",
    "                s_layer=i\n",
    "                break\n",
    "        print(s_layer)        \n",
    "        nw,ow = ret_new_weights(layer.weight,choices)\n",
    "        nb,ob = ret_new_bias(layer.bias,choices)\n",
    "        ow.requires_grad = True\n",
    "        ob.requires_grad = True\n",
    "        old_layer = nn.Linear(layer.in_features,layer.out_features-len(neg_index))\n",
    "        #print(ow.shape,old_layer)\n",
    "        old_layer.weight= nn.Parameter(ow)\n",
    "        old_layer.bias = nn.Parameter(ob)\n",
    "        #print(ow.requires_grad,ob.requires_grad)\n",
    "        new_layer,feature_bottleneck,old_split,skip_fc,int_metrics = return_layers(layer,choices,num_nodes= len(neg_index)+10,samp_size=10000,reg_epochs=100,verbose=True)\n",
    "        #new_layer,old_split,int_metrics = return_layers_data(loaders,model,layer,choices,num_nodes= len(neg_index)+10,samp_size=10000,reg_epochs=100,verbose=True)\n",
    "        layers = layers[:s_layer]+[old_layer,new_layer,feature_bottleneck,old_split,skip_fc]+layers[s_layer:]\n",
    "        layers= {str(i): layers[i] for i in range(len(layers))}\n",
    "        #print(layers)\n",
    "        _,_,architecture_array = return_arc_array(model.architecture_array,0,s_layer,perm)\n",
    "        model = assign_model(model,block,growth_block,GrowthModel(layers,architecture_array,act_on))\n",
    "    return model\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0c0a6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(model):\n",
    "    for i in range(len(model.blocks)):\n",
    "        #print(f\"Block {i} QKV {model.blocks[i].attn.qkv}\")\n",
    "        model.blocks[i].attn.qkv = GrowthStep(model.blocks[i].attn.qkv,act_on=False)\n",
    "        #print(f\"Block {i} proj {model.blocks[i].attn.proj}\")\n",
    "        model.blocks[i].attn.proj = GrowthStep(model.blocks[i].attn.proj,act_on=False)\n",
    "        #print(f\"Block {i} MLP fc1 {model.blocks[i].mlp.fc1}\")\n",
    "        model.blocks[i].mlp.fc1 = GrowthStep(model.blocks[i].mlp.fc1)\n",
    "        #print(f\"Block {i} MLP fc2 {model.blocks[i].mlp.fc2}\")\n",
    "        model.blocks[i].mlp.fc2 = GrowthStep(model.blocks[i].mlp.fc2)\n",
    "    return model\n",
    "    # print(model.blocks[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4a53091f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GrowthStep(model,follow_neg=True,nodes=5,act_on=True):\n",
    "    layers =get_all_linear_layers(model,\"list\")\n",
    "    s_layer,layer,neg_index=calculate_max_layer(layers)\n",
    "    if follow_neg==False:\n",
    "        neg_index = neg_index[:nodes]\n",
    "    if len(neg_index) > layer.out_features/2:\n",
    "        neg_index = neg_index[:len(neg_index)//2]\n",
    "    \n",
    "    choices=[n[0] for n in neg_index]\n",
    "    choices.sort()\n",
    "    \n",
    "    print(choices)\n",
    "    chosen_index= c_index(layer.out_features,choices)\n",
    "    perm = permutation(choices,layer.out_features)\n",
    "    print(s_layer)\n",
    "    layer=layers.pop(s_layer)\n",
    "    nw,ow = ret_new_weights(layer.weight,choices)\n",
    "    nb,ob = ret_new_bias(layer.bias,choices)\n",
    "    ow.requires_grad = True\n",
    "    ob.requires_grad = True\n",
    "    old_layer = nn.Linear(layer.in_features,layer.out_features-len(neg_index))\n",
    "    #print(ow.shape,old_layer)\n",
    "    old_layer.weight= nn.Parameter(ow)\n",
    "    old_layer.bias = nn.Parameter(ob)\n",
    "    #print(ow.requires_grad,ob.requires_grad)\n",
    "    new_layer,feature_bottleneck,old_split,skip_fc,int_metrics = return_layers(layer,choices,num_nodes= len(neg_index)+10,samp_size=10000,reg_epochs=1000,verbose=True)\n",
    "    #new_layer,old_split,int_metrics = return_layers_data(loaders,model,layer,choices,num_nodes= len(neg_index)+10,samp_size=10000,reg_epochs=100,verbose=True)\n",
    "    layers = layers[:s_layer]+[old_layer,new_layer,feature_bottleneck,old_split,skip_fc]+layers[s_layer:]\n",
    "    layers= {str(i): layers[i] for i in range(len(layers))}\n",
    "    #print(layers)\n",
    "    _,_,architecture_array = return_arc_array(model.architecture_array,0,s_layer,perm)\n",
    "    model1 = GrowthModel(layers,architecture_array,act_on)\n",
    "    return model1, nw,ow,nb,ob,perm,int_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "350b0ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_arc_array(a_array,i_num,sel_layer,positional):\n",
    "    def create_numbered_arc_array(arc_array,init_num):\n",
    "        name_array=[]\n",
    "        arc_new_arr=[]\n",
    "        for i in range(len(arc_array)):\n",
    "            if arc_array[i] ==0:\n",
    "                name_array.append(init_num)\n",
    "                if init_num != sel_layer:\n",
    "                    arc_new_arr.append(0)\n",
    "                else:\n",
    "                    arc_new_arr.append([[0,0,0,0,0],positional])\n",
    "                init_num+=1\n",
    "            else:\n",
    "                named_child_array,init_num,narr = create_numbered_arc_array(arc_array[i][0],init_num)\n",
    "                #print(arc_array[i][1])\n",
    "                name_array.append([named_child_array,arc_array[i][1]])\n",
    "                arc_new_arr.append([narr,arc_array[i][1]])\n",
    "        return name_array,init_num,arc_new_arr\n",
    "    return create_numbered_arc_array(a_array,i_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bda0bf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[[[0, 1, 2, 3, 4], [1, 2]], 5, 6, 7, 8], [3, 4]], 9],\n",
       " 10,\n",
       " [[[[[0, 0, 0, 0, 0], [1, 2]], 0, [[0, 0, 0, 0, 0], [5, 6]], 0, 0], [3, 4]],\n",
       "  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_arc_array([[[[[0,0,0,0,0],[1,2]],0,0,0,0],[3,4]],0],0,6,[5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1c301e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5910800\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.numel() for p in model.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2cc0090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7134032\n"
     ]
    }
   ],
   "source": [
    "print(sum([p.numel() for p in m2.parameters()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "901cb636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2304, 768])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l[0].weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340b0ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,3,4],[0,7,9,8]])\n",
    "r,c=a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550255b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.min(a,axis=0).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a56b3fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 0, 1, 2],\n",
       "        [3, 5, 7, 6]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.flatten().argsort().view(2,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4552e272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7//4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "486c452d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7- (7//4 *c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "711fa98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_1 = torch.load(r\"C:\\Users\\akash\\Downloads\\1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e47c8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_2 =torch.load(r\"C:\\Users\\akash\\Downloads\\2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ecffb68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=95, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=107, bias=True)\n",
       "            (2): Linear(in_features=107, out_features=53, bias=True)\n",
       "            (3): Linear(in_features=53, out_features=97, bias=True)\n",
       "            (4): Linear(in_features=107, out_features=97, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=94, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=108, bias=True)\n",
       "            (2): Linear(in_features=108, out_features=54, bias=True)\n",
       "            (3): Linear(in_features=54, out_features=98, bias=True)\n",
       "            (4): Linear(in_features=108, out_features=98, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9b18105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=105, bias=True)\n",
       "            (2): Linear(in_features=105, out_features=52, bias=True)\n",
       "            (3): Linear(in_features=52, out_features=95, bias=True)\n",
       "            (4): Linear(in_features=105, out_features=95, bias=True)\n",
       "            (5): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (6): Linear(in_features=768, out_features=117, bias=True)\n",
       "            (7): Linear(in_features=117, out_features=58, bias=True)\n",
       "            (8): Linear(in_features=58, out_features=107, bias=True)\n",
       "            (9): Linear(in_features=117, out_features=107, bias=True)\n",
       "            (10): Linear(in_features=107, out_features=0, bias=True)\n",
       "            (11): Linear(in_features=107, out_features=63, bias=True)\n",
       "            (12): Linear(in_features=63, out_features=31, bias=True)\n",
       "            (13): Linear(in_features=31, out_features=53, bias=True)\n",
       "            (14): Linear(in_features=63, out_features=53, bias=True)\n",
       "            (15): Linear(in_features=53, out_features=0, bias=True)\n",
       "            (16): Linear(in_features=53, out_features=107, bias=True)\n",
       "            (17): Linear(in_features=107, out_features=53, bias=True)\n",
       "            (18): Linear(in_features=53, out_features=97, bias=True)\n",
       "            (19): Linear(in_features=107, out_features=97, bias=True)\n",
       "            (20): Linear(in_features=107, out_features=0, bias=True)\n",
       "            (21): Linear(in_features=107, out_features=107, bias=True)\n",
       "            (22): Linear(in_features=107, out_features=53, bias=True)\n",
       "            (23): Linear(in_features=53, out_features=97, bias=True)\n",
       "            (24): Linear(in_features=107, out_features=97, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=96, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=106, bias=True)\n",
       "            (2): Linear(in_features=106, out_features=53, bias=True)\n",
       "            (3): Linear(in_features=53, out_features=96, bias=True)\n",
       "            (4): Linear(in_features=106, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=88, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=114, bias=True)\n",
       "            (2): Linear(in_features=114, out_features=57, bias=True)\n",
       "            (3): Linear(in_features=57, out_features=104, bias=True)\n",
       "            (4): Linear(in_features=114, out_features=104, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=104, bias=True)\n",
       "            (2): Linear(in_features=104, out_features=52, bias=True)\n",
       "            (3): Linear(in_features=52, out_features=94, bias=True)\n",
       "            (4): Linear(in_features=104, out_features=94, bias=True)\n",
       "            (5): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (6): Linear(in_features=768, out_features=118, bias=True)\n",
       "            (7): Linear(in_features=118, out_features=59, bias=True)\n",
       "            (8): Linear(in_features=59, out_features=108, bias=True)\n",
       "            (9): Linear(in_features=118, out_features=108, bias=True)\n",
       "            (10): Linear(in_features=108, out_features=0, bias=True)\n",
       "            (11): Linear(in_features=108, out_features=64, bias=True)\n",
       "            (12): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (13): Linear(in_features=32, out_features=54, bias=True)\n",
       "            (14): Linear(in_features=64, out_features=54, bias=True)\n",
       "            (15): Linear(in_features=54, out_features=0, bias=True)\n",
       "            (16): Linear(in_features=54, out_features=108, bias=True)\n",
       "            (17): Linear(in_features=108, out_features=54, bias=True)\n",
       "            (18): Linear(in_features=54, out_features=98, bias=True)\n",
       "            (19): Linear(in_features=108, out_features=98, bias=True)\n",
       "            (20): Linear(in_features=108, out_features=0, bias=True)\n",
       "            (21): Linear(in_features=108, out_features=108, bias=True)\n",
       "            (22): Linear(in_features=108, out_features=54, bias=True)\n",
       "            (23): Linear(in_features=54, out_features=98, bias=True)\n",
       "            (24): Linear(in_features=108, out_features=98, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d59afb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "one = torch.load(r\"C:\\Users\\akash\\Downloads\\1.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11d0d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=torch.rand((1,3,224,224)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b143927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "op = one(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f86a9bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "826e19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "two = torch.load(r\"C:\\Users\\akash\\Downloads\\2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e4205b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 dim 1 must match mat2 dim 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m op2\u001b[38;5;241m=\u001b[39m\u001b[43mtwo\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\timm\\models\\vision_transformer.py:281\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 281\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    282\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(x)\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\timm\\models\\vision_transformer.py:275\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    272\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_drop(x)\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 275\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    277\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x[:, \u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\timm\\models\\vision_transformer.py:142\u001b[0m, in \u001b[0;36mBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    141\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)))\n\u001b[1;32m--> 142\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\timm\\models\\vision_transformer.py:93\u001b[0m, in \u001b[0;36mMlp.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     91\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[0;32m     92\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[1;32m---> 93\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop(x)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\Growth.py:127\u001b[0m, in \u001b[0;36mGrowthModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    124\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_dict[\u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_count)](x)\n\u001b[0;32m    125\u001b[0m         \u001b[38;5;66;03m#print(x.device)\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m  \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchitecture_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marchitecture_array\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfinal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;66;03m#print(x.device)\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m#print(\"Final Output\",torch.sum(x))\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m#print(x.shape)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\Growth.py:26\u001b[0m, in \u001b[0;36mGrowthModel.module\u001b[1;34m(self, arc_array, x, final)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m#old_layer\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43marchitecture_array\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;66;03m#print(\"old Layer\",torch.sum(x1))\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture_array[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m#new_layer\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m#print(1,self.layer_dict[str(self.layer_count)])\u001b[39;00m\n",
      "File \u001b[1;32m~\\FILES\\Research\\Growth\\Transformer\\deit-main\\Growth.py:20\u001b[0m, in \u001b[0;36mGrowthModel.module\u001b[1;34m(self, arc_array, x, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#print([i for i in range(len(chosen_indices)) if chosen_indices[i]==1])\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m architecture_array[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#old_layer\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_count\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#print(0,self.layer_dict[str(self.layer_count)])\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#print(\"old Layer\",torch.sum(x1))\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_count\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\modules\\module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    725\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 727\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[0;32m    729\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[0;32m    730\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m    731\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\modules\\linear.py:93\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\torch_norway\\lib\\site-packages\\torch\\nn\\functional.py:1692\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1690\u001b[0m     ret \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39maddmm(bias, \u001b[38;5;28minput\u001b[39m, weight\u001b[38;5;241m.\u001b[39mt())\n\u001b[0;32m   1691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1692\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1694\u001b[0m         output \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m bias\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 dim 1 must match mat2 dim 0"
     ]
    }
   ],
   "source": [
    "op2=two(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e2bac85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=105, bias=True)\n",
       "            (2): Linear(in_features=105, out_features=52, bias=True)\n",
       "            (3): Linear(in_features=52, out_features=95, bias=True)\n",
       "            (4): Linear(in_features=105, out_features=95, bias=True)\n",
       "            (5): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (6): Linear(in_features=768, out_features=117, bias=True)\n",
       "            (7): Linear(in_features=117, out_features=58, bias=True)\n",
       "            (8): Linear(in_features=58, out_features=107, bias=True)\n",
       "            (9): Linear(in_features=117, out_features=107, bias=True)\n",
       "            (10): Linear(in_features=107, out_features=0, bias=True)\n",
       "            (11): Linear(in_features=107, out_features=63, bias=True)\n",
       "            (12): Linear(in_features=63, out_features=31, bias=True)\n",
       "            (13): Linear(in_features=31, out_features=53, bias=True)\n",
       "            (14): Linear(in_features=63, out_features=53, bias=True)\n",
       "            (15): Linear(in_features=53, out_features=0, bias=True)\n",
       "            (16): Linear(in_features=53, out_features=107, bias=True)\n",
       "            (17): Linear(in_features=107, out_features=53, bias=True)\n",
       "            (18): Linear(in_features=53, out_features=97, bias=True)\n",
       "            (19): Linear(in_features=107, out_features=97, bias=True)\n",
       "            (20): Linear(in_features=107, out_features=0, bias=True)\n",
       "            (21): Linear(in_features=107, out_features=107, bias=True)\n",
       "            (22): Linear(in_features=107, out_features=53, bias=True)\n",
       "            (23): Linear(in_features=53, out_features=97, bias=True)\n",
       "            (24): Linear(in_features=107, out_features=97, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=96, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=106, bias=True)\n",
       "            (2): Linear(in_features=106, out_features=53, bias=True)\n",
       "            (3): Linear(in_features=53, out_features=96, bias=True)\n",
       "            (4): Linear(in_features=106, out_features=96, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=88, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=114, bias=True)\n",
       "            (2): Linear(in_features=114, out_features=57, bias=True)\n",
       "            (3): Linear(in_features=57, out_features=104, bias=True)\n",
       "            (4): Linear(in_features=114, out_features=104, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=104, bias=True)\n",
       "            (2): Linear(in_features=104, out_features=52, bias=True)\n",
       "            (3): Linear(in_features=52, out_features=94, bias=True)\n",
       "            (4): Linear(in_features=104, out_features=94, bias=True)\n",
       "            (5): Linear(in_features=768, out_features=0, bias=True)\n",
       "            (6): Linear(in_features=768, out_features=118, bias=True)\n",
       "            (7): Linear(in_features=118, out_features=59, bias=True)\n",
       "            (8): Linear(in_features=59, out_features=108, bias=True)\n",
       "            (9): Linear(in_features=118, out_features=108, bias=True)\n",
       "            (10): Linear(in_features=108, out_features=0, bias=True)\n",
       "            (11): Linear(in_features=108, out_features=64, bias=True)\n",
       "            (12): Linear(in_features=64, out_features=32, bias=True)\n",
       "            (13): Linear(in_features=32, out_features=54, bias=True)\n",
       "            (14): Linear(in_features=64, out_features=54, bias=True)\n",
       "            (15): Linear(in_features=54, out_features=0, bias=True)\n",
       "            (16): Linear(in_features=54, out_features=108, bias=True)\n",
       "            (17): Linear(in_features=108, out_features=54, bias=True)\n",
       "            (18): Linear(in_features=54, out_features=98, bias=True)\n",
       "            (19): Linear(in_features=108, out_features=98, bias=True)\n",
       "            (20): Linear(in_features=108, out_features=0, bias=True)\n",
       "            (21): Linear(in_features=108, out_features=108, bias=True)\n",
       "            (22): Linear(in_features=108, out_features=54, bias=True)\n",
       "            (23): Linear(in_features=54, out_features=98, bias=True)\n",
       "            (24): Linear(in_features=108, out_features=98, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "526b4533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 192, kernel_size=(16, 16), stride=(16, 16))\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (blocks): ModuleList(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): Identity()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=95, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=107, bias=True)\n",
       "            (2): Linear(in_features=107, out_features=53, bias=True)\n",
       "            (3): Linear(in_features=53, out_features=97, bias=True)\n",
       "            (4): Linear(in_features=107, out_features=97, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=94, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=108, bias=True)\n",
       "            (2): Linear(in_features=108, out_features=54, bias=True)\n",
       "            (3): Linear(in_features=54, out_features=98, bias=True)\n",
       "            (4): Linear(in_features=108, out_features=98, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=576, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (drop_path): DropPath()\n",
       "      (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (act): GELU()\n",
       "        (fc2): GrowthModel(\n",
       "          (layer_dict): ModuleDict(\n",
       "            (0): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Linear(in_features=192, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ddb58c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
